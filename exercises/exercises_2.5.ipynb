{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import numpy as np\n",
    "import tvm\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm.script import tir as T\n",
    "from matplotlib import pyplot as plt \n",
    "import math \n",
    "from tvm.script import tir "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5. Exercises for TensorIR¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.5.1: Exercise using j_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyModule:\n",
    "    @T.prim_func\n",
    "    def mm_relu(A: T.Buffer[(128, 128), \"float32\"],\n",
    "                B: T.Buffer[(128, 128), \"float32\"],\n",
    "                C: T.Buffer[(128, 128), \"float32\"]):\n",
    "        T.func_attr({\"global_symbol\": \"mm_relu\", \"tir.noalias\": True})\n",
    "        Y = T.alloc_buffer((128, 128), dtype=\"float32\")\n",
    "        for i, j, k in T.grid(128, 128, 128):\n",
    "            with T.block(\"Y\"):\n",
    "                vi = T.axis.spatial(128, i)\n",
    "                vj = T.axis.spatial(128, j)\n",
    "                vk = T.axis.reduce(128, k)\n",
    "                with T.init():\n",
    "                    Y[vi, vj] = T.float32(0)\n",
    "                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]\n",
    "        for i, j in T.grid(128, 128):\n",
    "            with T.block(\"C\"):\n",
    "                vi = T.axis.spatial(128, i)\n",
    "                vj = T.axis.spatial(128, j)\n",
    "                C[vi, vj] = T.max(Y[vi, vj], T.float32(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(mod, jfactor):\n",
    "    sch = tvm.tir.Schedule(mod)\n",
    "    block_Y = sch.get_block(\"Y\", func_name=\"mm_relu\")\n",
    "    i, j, k = sch.get_loops(block_Y)\n",
    "    j0, j1 = sch.split(j, factors=[None, jfactor])\n",
    "    sch.reorder(j0, k, j1)\n",
    "    block_C = sch.get_block(\"C\", \"mm_relu\")\n",
    "    sch.reverse_compute_at(block_C, j0)\n",
    "    return sch.mod\n",
    "\n",
    "def try_with_jfactor(jfactor: int):\n",
    "    dtype = \"float32\"\n",
    "    a_np = np.random.rand(128, 128).astype(dtype)\n",
    "    b_np = np.random.rand(128, 128).astype(dtype)\n",
    "    a_nd = tvm.nd.array(a_np)\n",
    "    b_nd = tvm.nd.array(b_np)\n",
    "    c_nd = tvm.nd.empty((128, 128), dtype=\"float32\")\n",
    "    type(c_nd)\n",
    "\n",
    "    mod_transformed = transform(MyModule, jfactor=jfactor)\n",
    "\n",
    "    rt_lib_transformed = tvm.build(mod_transformed, \"llvm\")\n",
    "    f_timer_transformed = rt_lib_transformed.time_evaluator(\"mm_relu\", tvm.cpu())\n",
    "    return f_timer_transformed(a_nd, b_nd, c_nd).mean, mod_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jfactor 1: 0.00413448124 (s)\n",
      "jfactor 2: 0.0007935012499999999 (s)\n",
      "jfactor 3: 0.00068636875 (s)\n",
      "jfactor 4: 0.00042908458 (s)\n",
      "jfactor 5: 0.00045317416999999995 (s)\n",
      "jfactor 6: 0.00041749585 (s)\n",
      "jfactor 7: 0.0004534608400000001 (s)\n",
      "jfactor 8: 0.00026729375 (s)\n",
      "jfactor 9: 0.00045268666 (s)\n",
      "jfactor 10: 0.00024596919 (s)\n",
      "jfactor 11: 0.00029234834 (s)\n",
      "jfactor 12: 0.00021513289 (s)\n",
      "jfactor 13: 0.00021232376000000002 (s)\n",
      "jfactor 14: 0.00026907833 (s)\n",
      "jfactor 15: 0.00030697209 (s)\n",
      "jfactor 16: 0.00039187414999999995 (s)\n",
      "jfactor 17: 0.00030614876999999997 (s)\n",
      "jfactor 18: 0.00026941626 (s)\n",
      "jfactor 19: 0.00021991001 (s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb#ch0000024?line=4'>5</a>\u001b[0m times \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb#ch0000024?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m repeats \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb#ch0000024?line=6'>7</a>\u001b[0m     time, mod_transformed \u001b[39m=\u001b[39m try_with_jfactor(jfactor)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb#ch0000024?line=7'>8</a>\u001b[0m     times\u001b[39m.\u001b[39mappend(time)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb#ch0000024?line=8'>9</a>\u001b[0m mean_benchmark \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(times) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(times)\n",
      "\u001b[1;32m/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb Cell 5'\u001b[0m in \u001b[0;36mtry_with_jfactor\u001b[0;34m(jfactor)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb#ch0000023?line=19'>20</a>\u001b[0m mod_transformed \u001b[39m=\u001b[39m transform(MyModule, jfactor\u001b[39m=\u001b[39mjfactor)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb#ch0000023?line=21'>22</a>\u001b[0m rt_lib_transformed \u001b[39m=\u001b[39m tvm\u001b[39m.\u001b[39mbuild(mod_transformed, \u001b[39m\"\u001b[39m\u001b[39mllvm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb#ch0000023?line=22'>23</a>\u001b[0m f_timer_transformed \u001b[39m=\u001b[39m rt_lib_transformed\u001b[39m.\u001b[39;49mtime_evaluator(\u001b[39m\"\u001b[39;49m\u001b[39mmm_relu\u001b[39;49m\u001b[39m\"\u001b[39;49m, tvm\u001b[39m.\u001b[39;49mcpu())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb#ch0000023?line=23'>24</a>\u001b[0m \u001b[39mreturn\u001b[39;00m f_timer_transformed(a_nd, b_nd, c_nd)\u001b[39m.\u001b[39mmean, mod_transformed\n",
      "File \u001b[0;32m~/Desktop/dev_tvm/tvm/python/tvm/runtime/module.py:335\u001b[0m, in \u001b[0;36mModule.time_evaluator\u001b[0;34m(self, func_name, dev, number, repeat, min_repeat_ms, cooldown_interval_ms, repeats_to_cooldown, f_preproc)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/runtime/module.py?line=283'>284</a>\u001b[0m \u001b[39m\"\"\"Get an evaluator that measures time cost of running function.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/runtime/module.py?line=284'>285</a>\u001b[0m \n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/runtime/module.py?line=285'>286</a>\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/runtime/module.py?line=331'>332</a>\u001b[0m \u001b[39m    The ProfileResult reports `repeat` time costs in seconds.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/runtime/module.py?line=332'>333</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/runtime/module.py?line=333'>334</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/runtime/module.py?line=334'>335</a>\u001b[0m     feval \u001b[39m=\u001b[39m _ffi_api\u001b[39m.\u001b[39;49mRPCTimeEvaluator(\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/runtime/module.py?line=335'>336</a>\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/runtime/module.py?line=336'>337</a>\u001b[0m         func_name,\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/runtime/module.py?line=337'>338</a>\u001b[0m         dev\u001b[39m.\u001b[39;49mdevice_type,\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/runtime/module.py?line=338'>339</a>\u001b[0m         dev\u001b[39m.\u001b[39;49mdevice_id,\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/runtime/module.py?line=339'>340</a>\u001b[0m         number,\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/runtime/module.py?line=340'>341</a>\u001b[0m         repeat,\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/runtime/module.py?line=341'>342</a>\u001b[0m         min_repeat_ms,\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/runtime/module.py?line=342'>343</a>\u001b[0m         cooldown_interval_ms,\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/runtime/module.py?line=343'>344</a>\u001b[0m         repeats_to_cooldown,\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/runtime/module.py?line=344'>345</a>\u001b[0m         f_preproc,\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/runtime/module.py?line=345'>346</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/runtime/module.py?line=347'>348</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mevaluator\u001b[39m(\u001b[39m*\u001b[39margs):\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/runtime/module.py?line=348'>349</a>\u001b[0m         \u001b[39m\"\"\"Internal wrapped evaluator.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mean_benchmarks = []\n",
    "jfactors = []\n",
    "\n",
    "for jfactor in range(1, 129):\n",
    "    times = []\n",
    "    for repeats in range(10):\n",
    "        time, mod_transformed = try_with_jfactor(jfactor)\n",
    "        times.append(time)\n",
    "    mean_benchmark = sum(times) / len(times)\n",
    "    print(f\"jfactor {jfactor}: {mean_benchmark} (s)\")\n",
    "    mean_benchmarks.append(mean_benchmark)\n",
    "    jfactors.append(jfactor)\n",
    "\n",
    "log_mean_benchmarks = [math.log(d) for d in mean_benchmarks]\n",
    "plt.scatter(jfactors, log_mean_benchmarks)\n",
    "\n",
    "log_mean_benchmarks = [(log_mean_benchmarks[i], i + 1) for i in range(len(log_mean_benchmarks))]\n",
    "log_mean_benchmarks = sorted(log_mean_benchmarks)\n",
    "print(log_mean_benchmarks)\n",
    "\n",
    "# 22 jfactor, 128, and 64 are the fastest! Weird that 22 is so fast...\n",
    "# May also be imperfect breaking of the loops \n",
    "# Also shape of graph is weird... perhaps related to some kind of codegen thing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "# from tvm.script import tir as T\n",
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def mm_relu(A: T.Buffer[(128, 128), \"float32\"], B: T.Buffer[(128, 128), \"float32\"], C: T.Buffer[(128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        T.func_attr({\"global_symbol\": \"mm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with T.block(\"root\")\n",
      "        Y = T.alloc_buffer([128, 128], dtype=\"float32\")\n",
      "        for i, j_0 in T.grid(128, 6):\n",
      "            for k, j_1 in T.grid(128, 22):\n",
      "                with T.block(\"Y\"):\n",
      "                    vi = T.axis.spatial(128, i)\n",
      "                    vj = T.axis.spatial(128, j_0 * 22 + j_1)\n",
      "                    vk = T.axis.reduce(128, k)\n",
      "                    T.where(j_0 * 22 + j_1 < 128)\n",
      "                    T.reads(A[vi, vk], B[vk, vj])\n",
      "                    T.writes(Y[vi, vj])\n",
      "                    with T.init():\n",
      "                        Y[vi, vj] = T.float32(0)\n",
      "                    Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]\n",
      "            for ax0 in T.serial(22):\n",
      "                with T.block(\"C\"):\n",
      "                    vi = T.axis.spatial(128, i)\n",
      "                    vj = T.axis.spatial(128, j_0 * 22 + ax0)\n",
      "                    T.where(j_0 * 22 + ax0 < 128)\n",
      "                    T.reads(Y[vi, vj])\n",
      "                    T.writes(C[vi, vj])\n",
      "                    C[vi, vj] = T.max(Y[vi, vj], T.float32(0))\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "23\n",
      "# from tvm.script import tir as T\n",
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def mm_relu(A: T.Buffer[(128, 128), \"float32\"], B: T.Buffer[(128, 128), \"float32\"], C: T.Buffer[(128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        T.func_attr({\"global_symbol\": \"mm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with T.block(\"root\")\n",
      "        Y = T.alloc_buffer([128, 128], dtype=\"float32\")\n",
      "        for i, j_0 in T.grid(128, 6):\n",
      "            for k, j_1 in T.grid(128, 23):\n",
      "                with T.block(\"Y\"):\n",
      "                    vi = T.axis.spatial(128, i)\n",
      "                    vj = T.axis.spatial(128, j_0 * 23 + j_1)\n",
      "                    vk = T.axis.reduce(128, k)\n",
      "                    T.where(j_0 * 23 + j_1 < 128)\n",
      "                    T.reads(A[vi, vk], B[vk, vj])\n",
      "                    T.writes(Y[vi, vj])\n",
      "                    with T.init():\n",
      "                        Y[vi, vj] = T.float32(0)\n",
      "                    Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]\n",
      "            for ax0 in T.serial(23):\n",
      "                with T.block(\"C\"):\n",
      "                    vi = T.axis.spatial(128, i)\n",
      "                    vj = T.axis.spatial(128, j_0 * 23 + ax0)\n",
      "                    T.where(j_0 * 23 + ax0 < 128)\n",
      "                    T.reads(Y[vi, vj])\n",
      "                    T.writes(C[vi, vj])\n",
      "                    C[vi, vj] = T.max(Y[vi, vj], T.float32(0))\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "21\n",
      "# from tvm.script import tir as T\n",
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def mm_relu(A: T.Buffer[(128, 128), \"float32\"], B: T.Buffer[(128, 128), \"float32\"], C: T.Buffer[(128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        T.func_attr({\"global_symbol\": \"mm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with T.block(\"root\")\n",
      "        Y = T.alloc_buffer([128, 128], dtype=\"float32\")\n",
      "        for i, j_0 in T.grid(128, 7):\n",
      "            for k, j_1 in T.grid(128, 21):\n",
      "                with T.block(\"Y\"):\n",
      "                    vi = T.axis.spatial(128, i)\n",
      "                    vj = T.axis.spatial(128, j_0 * 21 + j_1)\n",
      "                    vk = T.axis.reduce(128, k)\n",
      "                    T.where(j_0 * 21 + j_1 < 128)\n",
      "                    T.reads(A[vi, vk], B[vk, vj])\n",
      "                    T.writes(Y[vi, vj])\n",
      "                    with T.init():\n",
      "                        Y[vi, vj] = T.float32(0)\n",
      "                    Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]\n",
      "            for ax0 in T.serial(21):\n",
      "                with T.block(\"C\"):\n",
      "                    vi = T.axis.spatial(128, i)\n",
      "                    vj = T.axis.spatial(128, j_0 * 21 + ax0)\n",
      "                    T.where(j_0 * 21 + ax0 < 128)\n",
      "                    T.reads(Y[vi, vj])\n",
      "                    T.writes(C[vi, vj])\n",
      "                    C[vi, vj] = T.max(Y[vi, vj], T.float32(0))\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "20\n",
      "# from tvm.script import tir as T\n",
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def mm_relu(A: T.Buffer[(128, 128), \"float32\"], B: T.Buffer[(128, 128), \"float32\"], C: T.Buffer[(128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        T.func_attr({\"global_symbol\": \"mm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with T.block(\"root\")\n",
      "        Y = T.alloc_buffer([128, 128], dtype=\"float32\")\n",
      "        for i, j_0 in T.grid(128, 7):\n",
      "            for k, j_1 in T.grid(128, 20):\n",
      "                with T.block(\"Y\"):\n",
      "                    vi = T.axis.spatial(128, i)\n",
      "                    vj = T.axis.spatial(128, j_0 * 20 + j_1)\n",
      "                    vk = T.axis.reduce(128, k)\n",
      "                    T.where(j_0 * 20 + j_1 < 128)\n",
      "                    T.reads(A[vi, vk], B[vk, vj])\n",
      "                    T.writes(Y[vi, vj])\n",
      "                    with T.init():\n",
      "                        Y[vi, vj] = T.float32(0)\n",
      "                    Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]\n",
      "            for ax0 in T.serial(20):\n",
      "                with T.block(\"C\"):\n",
      "                    vi = T.axis.spatial(128, i)\n",
      "                    vj = T.axis.spatial(128, j_0 * 20 + ax0)\n",
      "                    T.where(j_0 * 20 + ax0 < 128)\n",
      "                    T.reads(Y[vi, vj])\n",
      "                    T.writes(C[vi, vj])\n",
      "                    C[vi, vj] = T.max(Y[vi, vj], T.float32(0))\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "128\n",
      "# from tvm.script import tir as T\n",
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def mm_relu(A: T.Buffer[(128, 128), \"float32\"], B: T.Buffer[(128, 128), \"float32\"], C: T.Buffer[(128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        T.func_attr({\"global_symbol\": \"mm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with T.block(\"root\")\n",
      "        Y = T.alloc_buffer([128, 128], dtype=\"float32\")\n",
      "        for i, j_0 in T.grid(128, 1):\n",
      "            for k, j_1 in T.grid(128, 128):\n",
      "                with T.block(\"Y\"):\n",
      "                    vi = T.axis.spatial(128, i)\n",
      "                    vj = T.axis.spatial(128, j_0 * 128 + j_1)\n",
      "                    vk = T.axis.reduce(128, k)\n",
      "                    T.reads(A[vi, vk], B[vk, vj])\n",
      "                    T.writes(Y[vi, vj])\n",
      "                    with T.init():\n",
      "                        Y[vi, vj] = T.float32(0)\n",
      "                    Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]\n",
      "            for ax0 in T.serial(128):\n",
      "                with T.block(\"C\"):\n",
      "                    vi, vj = T.axis.remap(\"SS\", [i, ax0])\n",
      "                    T.reads(Y[vi, vj])\n",
      "                    T.writes(C[vi, vj])\n",
      "                    C[vi, vj] = T.max(Y[vi, vj], T.float32(0))\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "127\n",
      "# from tvm.script import tir as T\n",
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def mm_relu(A: T.Buffer[(128, 128), \"float32\"], B: T.Buffer[(128, 128), \"float32\"], C: T.Buffer[(128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        T.func_attr({\"global_symbol\": \"mm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with T.block(\"root\")\n",
      "        Y = T.alloc_buffer([128, 128], dtype=\"float32\")\n",
      "        for i, j_0 in T.grid(128, 2):\n",
      "            for k, j_1 in T.grid(128, 127):\n",
      "                with T.block(\"Y\"):\n",
      "                    vi = T.axis.spatial(128, i)\n",
      "                    vj = T.axis.spatial(128, j_0 * 127 + j_1)\n",
      "                    vk = T.axis.reduce(128, k)\n",
      "                    T.where(j_0 * 127 + j_1 < 128)\n",
      "                    T.reads(A[vi, vk], B[vk, vj])\n",
      "                    T.writes(Y[vi, vj])\n",
      "                    with T.init():\n",
      "                        Y[vi, vj] = T.float32(0)\n",
      "                    Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]\n",
      "            for ax0 in T.serial(127):\n",
      "                with T.block(\"C\"):\n",
      "                    vi = T.axis.spatial(128, i)\n",
      "                    vj = T.axis.spatial(128, j_0 * 127 + ax0)\n",
      "                    T.where(j_0 * 127 + ax0 < 128)\n",
      "                    T.reads(Y[vi, vj])\n",
      "                    T.writes(C[vi, vj])\n",
      "                    C[vi, vj] = T.max(Y[vi, vj], T.float32(0))\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "31\n",
      "# from tvm.script import tir as T\n",
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def mm_relu(A: T.Buffer[(128, 128), \"float32\"], B: T.Buffer[(128, 128), \"float32\"], C: T.Buffer[(128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        T.func_attr({\"global_symbol\": \"mm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with T.block(\"root\")\n",
      "        Y = T.alloc_buffer([128, 128], dtype=\"float32\")\n",
      "        for i, j_0 in T.grid(128, 5):\n",
      "            for k, j_1 in T.grid(128, 31):\n",
      "                with T.block(\"Y\"):\n",
      "                    vi = T.axis.spatial(128, i)\n",
      "                    vj = T.axis.spatial(128, j_0 * 31 + j_1)\n",
      "                    vk = T.axis.reduce(128, k)\n",
      "                    T.where(j_0 * 31 + j_1 < 128)\n",
      "                    T.reads(A[vi, vk], B[vk, vj])\n",
      "                    T.writes(Y[vi, vj])\n",
      "                    with T.init():\n",
      "                        Y[vi, vj] = T.float32(0)\n",
      "                    Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]\n",
      "            for ax0 in T.serial(31):\n",
      "                with T.block(\"C\"):\n",
      "                    vi = T.axis.spatial(128, i)\n",
      "                    vj = T.axis.spatial(128, j_0 * 31 + ax0)\n",
      "                    T.where(j_0 * 31 + ax0 < 128)\n",
      "                    T.reads(Y[vi, vj])\n",
      "                    T.writes(C[vi, vj])\n",
      "                    C[vi, vj] = T.max(Y[vi, vj], T.float32(0))\n",
      "    \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, mod_22 = try_with_jfactor(22)\n",
    "_, mod_23 = try_with_jfactor(23)\n",
    "_, mod_21 = try_with_jfactor(21)\n",
    "_, mod_20 = try_with_jfactor(20)\n",
    "_, mod_128 = try_with_jfactor(128)\n",
    "_, mod_127 = try_with_jfactor(127)\n",
    "_, mod_31 = try_with_jfactor(31)\n",
    "\n",
    "print(f\"22\\n{mod_22.script()}\\n\\n\")\n",
    "print(f\"23\\n{mod_23.script()}\\n\\n\")\n",
    "print(f\"21\\n{mod_21.script()}\\n\\n\")\n",
    "print(f\"20\\n{mod_20.script()}\\n\\n\")\n",
    "print(f\"128\\n{mod_128.script()}\\n\\n\")\n",
    "print(f\"127\\n{mod_127.script()}\\n\\n\")\n",
    "print(f\"31\\n{mod_31.script()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5.1.1. Example: Element-wise Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init data\n",
    "a = np.arange(16).reshape(4, 4)\n",
    "b = np.arange(16, 0, -1).reshape(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy version\n",
    "c_np = a + b\n",
    "c_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# low-level numpy version\n",
    "def lnumpy_add(a: np.ndarray, b: np.ndarray, c: np.ndarray):\n",
    "  for i in range(4):\n",
    "    for j in range(4):\n",
    "      c[i, j] = a[i, j] + b[i, j]\n",
    "c_lnumpy = np.empty((4, 4), dtype=np.int64)\n",
    "lnumpy_add(a, b, c_lnumpy)\n",
    "c_lnumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorIR version\n",
    "@tvm.script.ir_module\n",
    "class MyAdd:\n",
    "  @T.prim_func\n",
    "  def add(A: T.Buffer[(4, 4), \"int64\"],\n",
    "          B: T.Buffer[(4, 4), \"int64\"],\n",
    "          C: T.Buffer[(4, 4), \"int64\"]):\n",
    "    T.func_attr({\"global_symbol\": \"add\"})\n",
    "    for i, j in T.grid(4, 4):\n",
    "      with T.block(\"C\"):\n",
    "        vi = T.axis.spatial(4, i)\n",
    "        vj = T.axis.spatial(4, j)\n",
    "        C[vi, vj] = A[vi, vj] + B[vi, vj]\n",
    "\n",
    "rt_lib = tvm.build(MyAdd, target=\"llvm\")\n",
    "a_tvm = tvm.nd.array(a)\n",
    "b_tvm = tvm.nd.array(b)\n",
    "c_tvm = tvm.nd.array(np.empty((4, 4), dtype=np.int64))\n",
    "rt_lib[\"add\"](a_tvm, b_tvm, c_tvm)\n",
    "np.testing.assert_allclose(c_tvm.numpy(), c_np, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5.1.2. Exercise 1: Broadcast Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init data\n",
    "a = np.arange(16).reshape(4, 4)\n",
    "b = np.arange(4, 0, -1).reshape(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  4,  4,  4],\n",
       "       [ 8,  8,  8,  8],\n",
       "       [12, 12, 12, 12],\n",
       "       [16, 16, 16, 16]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy version\n",
    "c_np = a + b\n",
    "c_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyAdd:\n",
    "  @T.prim_func\n",
    "  def add(\n",
    "      A: T.Buffer[(4, 4), \"int64\"], \n",
    "      B: T.Buffer[(4), \"int64\"],\n",
    "      C: T.Buffer[(4, 4), \"int64\"]\n",
    "    ):\n",
    "        T.func_attr({\"global_symbol\": \"add\", \"tir.noalias\": True})\n",
    "        for i, j in T.grid(4, 4):\n",
    "            with T.block(\"C\"):\n",
    "                vi = T.axis.spatial(4, i)\n",
    "                vj = T.axis.spatial(4, j)\n",
    "                C[vi, vj] = A[vi, vj] + B[vj]\n",
    "\n",
    "rt_lib = tvm.build(MyAdd, target=\"llvm\")\n",
    "a_tvm = tvm.nd.array(a)\n",
    "b_tvm = tvm.nd.array(b)\n",
    "c_tvm = tvm.nd.array(np.empty((4, 4), dtype=np.int64))\n",
    "rt_lib[\"add\"](a_tvm, b_tvm, c_tvm)\n",
    "np.testing.assert_allclose(c_tvm.numpy(), c_np, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5.1.3. Exercise 2: 2D Convolution¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, CI, H, W, CO, K = 1, 1, 8, 8, 2, 3\n",
    "OUT_H, OUT_W = H - K + 1, W - K + 1\n",
    "data = np.arange(N*CI*H*W).reshape(N, CI, H, W)\n",
    "weight = np.arange(CO*CI*K*K).reshape(CO, CI, K, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[             474,              510,              546,\n",
       "                       582,              618,              654],\n",
       "         [             762,              798,              834,\n",
       "                       870,              906,              942],\n",
       "         [            1050,             1086,             1122,\n",
       "                      1158,             1194,             1230],\n",
       "         [            1338,             1374,             1410,\n",
       "                      1446,             1482,             1518],\n",
       "         [            1626,             1662,             1698,\n",
       "                      1734,             1770,             1806],\n",
       "         [            1914,             1950,             1986,\n",
       "                      2022,             2058,             2094]],\n",
       "\n",
       "        [[               0,                0, 8951137046626304,\n",
       "                         0,                0,                0],\n",
       "         [               0,                0,                0,\n",
       "                         0,                0,                0],\n",
       "         [               0,                0, 8972506656407552,\n",
       "                         0,                0,                0],\n",
       "         [               0,                0,                0,\n",
       "                         0,                0,                0],\n",
       "         [               0,                0,                0,\n",
       "                         0,                0,                0],\n",
       "         [               0,                0,                0,\n",
       "                         0,                0,                0]]]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch version\n",
    "import torch\n",
    "\n",
    "data_torch = torch.Tensor(data)\n",
    "weight_torch = torch.Tensor(weight)\n",
    "conv_torch = torch.nn.functional.conv2d(data_torch, weight_torch)\n",
    "conv_torch = conv_torch.numpy().astype(np.int64)\n",
    "conv_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyConv:\n",
    "  N, CI, H, W, CO, K = 1, 1, 8, 8, 2, 3\n",
    "  OUT_H, OUT_W = H - K + 1, W - K + 1\n",
    "  @T.prim_func\n",
    "  def conv(\n",
    "    in_img: T.Buffer[(1, 1, 8, 8), \"int64\"],\n",
    "    filters: T.Buffer[(2, 1, 3, 3), \"int64\"],\n",
    "    out_img: T.Buffer[(1, 2, 6, 6), \"int64\"]\n",
    "  ):\n",
    "    T.func_attr({\"global_symbol\": \"conv\", \"tir.noalias\": True})\n",
    "    for n, co, i, j, kh, kw in T.grid(1, 2, 6, 6, 3, 3):\n",
    "      with T.block(\"conv\"):\n",
    "        vn, vco, vi, vj, vkh, vkw = T.axis.remap(\"SSSSRR\", [n, co, i, j, kh, kw]) \n",
    "        with T.init():\n",
    "          out_img[vn, vco, vi, vj] = T.int64(0)\n",
    "        out_img[vn, vco, vi, vj] += in_img[0, 0, vi + vkh, vj + vkw] * filters[vco, 0, vkh, vkw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 474  510  546  582  618  654]\n",
      "   [ 762  798  834  870  906  942]\n",
      "   [1050 1086 1122 1158 1194 1230]\n",
      "   [1338 1374 1410 1446 1482 1518]\n",
      "   [1626 1662 1698 1734 1770 1806]\n",
      "   [1914 1950 1986 2022 2058 2094]]\n",
      "\n",
      "  [[1203 1320 1437 1554 1671 1788]\n",
      "   [2139 2256 2373 2490 2607 2724]\n",
      "   [3075 3192 3309 3426 3543 3660]\n",
      "   [4011 4128 4245 4362 4479 4596]\n",
      "   [4947 5064 5181 5298 5415 5532]\n",
      "   [5883 6000 6117 6234 6351 6468]]]]\n",
      "[[[[             474              510              546              582\n",
      "                 618              654]\n",
      "   [             762              798              834              870\n",
      "                 906              942]\n",
      "   [            1050             1086             1122             1158\n",
      "                1194             1230]\n",
      "   [            1338             1374             1410             1446\n",
      "                1482             1518]\n",
      "   [            1626             1662             1698             1734\n",
      "                1770             1806]\n",
      "   [            1914             1950             1986             2022\n",
      "                2058             2094]]\n",
      "\n",
      "  [[               0                0 8951137046626304                0\n",
      "                   0                0]\n",
      "   [               0                0                0                0\n",
      "                   0                0]\n",
      "   [               0                0 8972506656407552                0\n",
      "                   0                0]\n",
      "   [               0                0                0                0\n",
      "                   0                0]\n",
      "   [               0                0                0                0\n",
      "                   0                0]\n",
      "   [               0                0                0                0\n",
      "                   0                0]]]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nNot equal to tolerance rtol=1e-05, atol=0\n\nMismatched elements: 36 / 72 (50%)\nMax absolute difference: 8972506656404243\nMax relative difference: 1.\n x: array([[[[ 474,  510,  546,  582,  618,  654],\n         [ 762,  798,  834,  870,  906,  942],\n         [1050, 1086, 1122, 1158, 1194, 1230],...\n y: array([[[[             474,              510,              546,\n                       582,              618,              654],\n         [             762,              798,              834,...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb#ch0000015?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(conv_tvm\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb#ch0000015?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(conv_torch)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb#ch0000015?line=7'>8</a>\u001b[0m np\u001b[39m.\u001b[39;49mtesting\u001b[39m.\u001b[39;49massert_allclose(conv_tvm\u001b[39m.\u001b[39;49mnumpy(), conv_torch, rtol\u001b[39m=\u001b[39;49m\u001b[39m1e-5\u001b[39;49m)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/tvm_py3.8/lib/python3.8/site-packages/numpy/testing/_private/utils.py:844\u001b[0m, in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/miniforge3/envs/tvm_py3.8/lib/python3.8/site-packages/numpy/testing/_private/utils.py?line=839'>840</a>\u001b[0m         err_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(remarks)\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/miniforge3/envs/tvm_py3.8/lib/python3.8/site-packages/numpy/testing/_private/utils.py?line=840'>841</a>\u001b[0m         msg \u001b[39m=\u001b[39m build_err_msg([ox, oy], err_msg,\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/miniforge3/envs/tvm_py3.8/lib/python3.8/site-packages/numpy/testing/_private/utils.py?line=841'>842</a>\u001b[0m                             verbose\u001b[39m=\u001b[39mverbose, header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/miniforge3/envs/tvm_py3.8/lib/python3.8/site-packages/numpy/testing/_private/utils.py?line=842'>843</a>\u001b[0m                             names\u001b[39m=\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m), precision\u001b[39m=\u001b[39mprecision)\n\u001b[0;32m--> <a href='file:///Users/andrewzhaoluo/miniforge3/envs/tvm_py3.8/lib/python3.8/site-packages/numpy/testing/_private/utils.py?line=843'>844</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(msg)\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/miniforge3/envs/tvm_py3.8/lib/python3.8/site-packages/numpy/testing/_private/utils.py?line=844'>845</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/miniforge3/envs/tvm_py3.8/lib/python3.8/site-packages/numpy/testing/_private/utils.py?line=845'>846</a>\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtraceback\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=1e-05, atol=0\n\nMismatched elements: 36 / 72 (50%)\nMax absolute difference: 8972506656404243\nMax relative difference: 1.\n x: array([[[[ 474,  510,  546,  582,  618,  654],\n         [ 762,  798,  834,  870,  906,  942],\n         [1050, 1086, 1122, 1158, 1194, 1230],...\n y: array([[[[             474,              510,              546,\n                       582,              618,              654],\n         [             762,              798,              834,..."
     ]
    }
   ],
   "source": [
    "rt_lib = tvm.build(MyConv, target=\"llvm\")\n",
    "data_tvm = tvm.nd.array(data)\n",
    "weight_tvm = tvm.nd.array(weight)\n",
    "conv_tvm = tvm.nd.array(np.empty((N, CO, OUT_H, OUT_W), dtype=np.int64))\n",
    "rt_lib[\"conv\"](data_tvm, weight_tvm, conv_tvm)\n",
    "print(conv_tvm.numpy())\n",
    "print(conv_torch)\n",
    "np.testing.assert_allclose(conv_tvm.numpy(), conv_torch, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5.2.2 Batch Matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnumpy_mm_relu_v2(A: np.ndarray, B: np.ndarray, C: np.ndarray):\n",
    "    Y = np.empty((16, 128, 128), dtype=\"float32\")\n",
    "    for n in range(16):\n",
    "        for i in range(128):\n",
    "            for j in range(128):\n",
    "                for k in range(128):\n",
    "                    if k == 0:\n",
    "                        Y[n, i, j] = 0\n",
    "                    Y[n, i, j] = Y[n, i, j] + A[n, i, k] * B[n, k, j]\n",
    "    for n in range(16):\n",
    "        for i in range(128):\n",
    "            for j in range(128):\n",
    "                C[n, i, j] = max(Y[n, i, j], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# from tvm.script import tir as T\n",
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def bmm_relu(A: T.Buffer[(16, 128, 128), \"float32\"], B: T.Buffer[(16, 128, 128), \"float32\"], C: T.Buffer[(16, 128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with T.block(\"root\")\n",
      "        intermediate = T.alloc_buffer([16, 128, 128], dtype=\"float32\")\n",
      "        for n, i, j, k in T.grid(16, 128, 128, 128):\n",
      "            with T.block(\"matmul\"):\n",
      "                vn, vi, vj, vk = T.axis.remap(\"SSSR\", [n, i, j, k])\n",
      "                T.reads(A[vn, vi, vk], B[vn, vk, vj])\n",
      "                T.writes(intermediate[vn, vi, vj])\n",
      "                with T.init():\n",
      "                    intermediate[vn, vi, vj] = T.float32(0)\n",
      "                intermediate[vn, vi, vj] = intermediate[vn, vi, vj] + A[vn, vi, vk] * B[vn, vk, vj]\n",
      "        for n, i, j in T.grid(16, 128, 128):\n",
      "            with T.block(\"relu\"):\n",
      "                vn, vi, vj = T.axis.remap(\"SSS\", [n, i, j])\n",
      "                T.reads(intermediate[vn, vi, vj])\n",
      "                T.writes(C[vn, vi, vj])\n",
      "                C[vn, vi, vj] = T.max(intermediate[vn, vi, vj], T.float32(0))\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyBmmRelu:\n",
    "  @T.prim_func\n",
    "  def bmm_relu(\n",
    "      A: T.Buffer[(16, 128, 128), \"float32\"],\n",
    "      B: T.Buffer[(16, 128, 128), \"float32\"],\n",
    "      C: T.Buffer[(16, 128, 128), \"float32\"]\n",
    "  ):\n",
    "    T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
    "\n",
    "    intermediate = T.alloc_buffer((16, 128, 128), \"float32\")\n",
    "    for n, i, j, k in T.grid(16, 128, 128, 128):\n",
    "      with T.block(\"matmul\"):\n",
    "        vn, vi, vj, vk = T.axis.remap(\"SSSR\", [n, i, j, k])\n",
    "        with T.init(): \n",
    "          intermediate[vn, vi, vj] = T.float32(0)\n",
    "        intermediate[vn, vi, vj] += A[vn, vi, vk] * B[vn, vk, vj]\n",
    "    for n, i, j in T.grid(16, 128, 128):\n",
    "      with T.block(\"relu\"):\n",
    "        vn, vi, vj = T.axis.remap(\"SSS\", [n, i, j])\n",
    "        C[vn, vi, vj] = T.max(intermediate[vn, vi, vj], T.float32(0))\n",
    "\n",
    "print(MyBmmRelu.script())\n",
    "#sch = tvm.tir.Schedule(MyBmmRelu)\n",
    "#IPython.display.Code(sch.mod.script(), language=\"python\")\n",
    "# Also please validate your result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original\n",
      "# from tvm.script import tir as T\n",
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def bmm_relu(A: T.Buffer[(16, 128, 128), \"float32\"], B: T.Buffer[(16, 128, 128), \"float32\"], C: T.Buffer[(16, 128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with T.block(\"root\")\n",
      "        intermediate = T.alloc_buffer([16, 128, 128], dtype=\"float32\")\n",
      "        for n, i, j, k in T.grid(16, 128, 128, 128):\n",
      "            with T.block(\"matmul\"):\n",
      "                vn, vi, vj, vk = T.axis.remap(\"SSSR\", [n, i, j, k])\n",
      "                T.reads(A[vn, vi, vk], B[vn, vk, vj])\n",
      "                T.writes(intermediate[vn, vi, vj])\n",
      "                with T.init():\n",
      "                    intermediate[vn, vi, vj] = T.float32(0)\n",
      "                intermediate[vn, vi, vj] = intermediate[vn, vi, vj] + A[vn, vi, vk] * B[vn, vk, vj]\n",
      "        for n, i, j in T.grid(16, 128, 128):\n",
      "            with T.block(\"relu\"):\n",
      "                vn, vi, vj = T.axis.remap(\"SSS\", [n, i, j])\n",
      "                T.reads(intermediate[vn, vi, vj])\n",
      "                T.writes(C[vn, vi, vj])\n",
      "                C[vn, vi, vj] = T.max(intermediate[vn, vi, vj], T.float32(0))\n",
      "    \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb#ch0000019?line=16'>17</a>\u001b[0m sch\u001b[39m.\u001b[39mvectorize(relu_j1)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb#ch0000019?line=18'>19</a>\u001b[0m matmul_n, matmul_i, matmul_j0, matmul_ax0, matmul_ax1 \u001b[39m=\u001b[39m sch\u001b[39m.\u001b[39mget_loops(matmul_block)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb#ch0000019?line=19'>20</a>\u001b[0m init_block \u001b[39m=\u001b[39m sch\u001b[39m.\u001b[39;49mdecompose_reduction(matmul_block, matmul_ax0)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb#ch0000019?line=21'>22</a>\u001b[0m sch\u001b[39m.\u001b[39mreorder(matmul_ax1, matmul_ax0)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb#ch0000019?line=22'>23</a>\u001b[0m ax1_0, ax1_1 \u001b[39m=\u001b[39m sch\u001b[39m.\u001b[39msplit(matmul_ax1, [\u001b[39m32\u001b[39m, \u001b[39m4\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/_type_checker.py:237\u001b[0m, in \u001b[0;36mwrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/_type_checker.py?line=230'>231</a>\u001b[0m \u001b[39mif\u001b[39;00m param\u001b[39m.\u001b[39mannotation \u001b[39m!=\u001b[39m inspect\u001b[39m.\u001b[39mSignature\u001b[39m.\u001b[39mempty:\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/_type_checker.py?line=231'>232</a>\u001b[0m     error_msg \u001b[39m=\u001b[39m _type_check(\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/_type_checker.py?line=232'>233</a>\u001b[0m         bound_args\u001b[39m.\u001b[39marguments[param\u001b[39m.\u001b[39mname],\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/_type_checker.py?line=233'>234</a>\u001b[0m         param\u001b[39m.\u001b[39mname,\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/_type_checker.py?line=234'>235</a>\u001b[0m         param\u001b[39m.\u001b[39mannotation,\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/_type_checker.py?line=235'>236</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/_type_checker.py?line=236'>237</a>\u001b[0m     \u001b[39mif\u001b[39;00m error_msg \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/_type_checker.py?line=237'>238</a>\u001b[0m         error_msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mIn \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00merror_msg\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/_type_checker.py?line=238'>239</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(error_msg)\n",
      "File \u001b[0;32m~/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py:1632\u001b[0m, in \u001b[0;36mdecompose_reduction\u001b[0;34m(self, block, loop)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1577'>1578</a>\u001b[0m \u001b[39m@type_checked\u001b[39m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1578'>1579</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecompose_reduction\u001b[39m(\u001b[39mself\u001b[39m, block: Union[BlockRV, \u001b[39mstr\u001b[39m], loop: LoopRV) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BlockRV:\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1579'>1580</a>\u001b[0m     \u001b[39m\"\"\"Decompose a reduction block into two separate blocks.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1580'>1581</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1581'>1582</a>\u001b[0m \u001b[39m    a) The init block, which is translated from the init statement of the reduction block;\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1582'>1583</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1583'>1584</a>\u001b[0m \u001b[39m    b) The update block, which is the original block without init statement.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1584'>1585</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1585'>1586</a>\u001b[0m \u001b[39m    The init block is inserted right before the given loop.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1586'>1587</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1587'>1588</a>\u001b[0m \u001b[39m    The schedule primitive requires:\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1588'>1589</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1589'>1590</a>\u001b[0m \u001b[39m    1) The input block is a reduction block.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1590'>1591</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1591'>1592</a>\u001b[0m \u001b[39m    2) The input loop is the ancestor of the block.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1592'>1593</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1593'>1594</a>\u001b[0m \u001b[39m    3) The input loop is not lower than all the loops related to reduce block var.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1594'>1595</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1595'>1596</a>\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1596'>1597</a>\u001b[0m \u001b[39m    ----------\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1597'>1598</a>\u001b[0m \u001b[39m    block : Union[BlockRV, str]\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1598'>1599</a>\u001b[0m \u001b[39m        The reduction block to be decomposed\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1599'>1600</a>\u001b[0m \u001b[39m    loop : LoopRV\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1600'>1601</a>\u001b[0m \u001b[39m        The loop above which the init block is inserted before.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1601'>1602</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1602'>1603</a>\u001b[0m \u001b[39m    Returns\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1603'>1604</a>\u001b[0m \u001b[39m    -------\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1604'>1605</a>\u001b[0m \u001b[39m    init_block : BlockRV\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1605'>1606</a>\u001b[0m \u001b[39m        The init block\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1606'>1607</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1607'>1608</a>\u001b[0m \u001b[39m    Examples\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1608'>1609</a>\u001b[0m \u001b[39m    --------\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1609'>1610</a>\u001b[0m \u001b[39m    Before decompose-reduction, in TensorIR, the IR is:\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1610'>1611</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1611'>1612</a>\u001b[0m \u001b[39m    .. code-block:: python\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1612'>1613</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1613'>1614</a>\u001b[0m \u001b[39m        @tvm.script.tir\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1614'>1615</a>\u001b[0m \u001b[39m        def before_decompose(a: ty.handle, c: ty.handle) -> None:\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1615'>1616</a>\u001b[0m \u001b[39m            A = tir.match_buffer(a, [128, 128])\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1616'>1617</a>\u001b[0m \u001b[39m            B = tir.match_buffer(b, [128, 128])\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1617'>1618</a>\u001b[0m \u001b[39m            C = tir.match_buffer(c, [128, 128])\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1618'>1619</a>\u001b[0m \u001b[39m            for i, j, k in tir.grid(128, 128, 128):\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1619'>1620</a>\u001b[0m \u001b[39m                with tir.block([128, 128, tir.reduce_axis(0, 128)], \"C\") as [vi, vj, vk]:\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1620'>1621</a>\u001b[0m \u001b[39m                    with tir.init():\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1621'>1622</a>\u001b[0m \u001b[39m                        C[vi, vj] = 0.0\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1622'>1623</a>\u001b[0m \u001b[39m                    C[vi, vj] = C[vi, vj] + A[vi, vk] * B[vj, vk]\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1623'>1624</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1624'>1625</a>\u001b[0m \u001b[39m    Create the schedule and do decompose-reduction with specified loop:\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1625'>1626</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1626'>1627</a>\u001b[0m \u001b[39m    .. code-block:: python\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1627'>1628</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1628'>1629</a>\u001b[0m \u001b[39m        sch = tir.Schedule(before_decompose)\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1629'>1630</a>\u001b[0m \u001b[39m        C = sch.get_block(\"C\")\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1630'>1631</a>\u001b[0m \u001b[39m        i, j, k = sch.get_loops(C)\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1631'>1632</a>\u001b[0m \u001b[39m        sch.decompose_reduction(C, i)\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1632'>1633</a>\u001b[0m \u001b[39m        print(tvm.script.asscript(sch.mod[\"main\"]))\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1633'>1634</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1634'>1635</a>\u001b[0m \u001b[39m    After applying decompose-reduction, the IR becomes:\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1635'>1636</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1636'>1637</a>\u001b[0m \u001b[39m    .. code-block:: python\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1637'>1638</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1638'>1639</a>\u001b[0m \u001b[39m        @tvm.script.tir\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1639'>1640</a>\u001b[0m \u001b[39m        def after_decompose(a: ty.handle, c: ty.handle) -> None:\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1640'>1641</a>\u001b[0m \u001b[39m            A = tir.match_buffer(a, [128, 128])\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1641'>1642</a>\u001b[0m \u001b[39m            B = tir.match_buffer(b, [128, 128])\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1642'>1643</a>\u001b[0m \u001b[39m            C = tir.match_buffer(c, [128, 128])\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1643'>1644</a>\u001b[0m \u001b[39m            for i in tir.serial(128):\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1644'>1645</a>\u001b[0m \u001b[39m                for j in tir.serial(128):\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1645'>1646</a>\u001b[0m \u001b[39m                    with tir.block([128, 128]) as [vi, vj]:\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1646'>1647</a>\u001b[0m \u001b[39m                        C[vi, vj] = 0.0\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1647'>1648</a>\u001b[0m \u001b[39m            for i, j, k in tir.grid(128, 128, 128):\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1648'>1649</a>\u001b[0m \u001b[39m                with tir.block([128, 128, tir.reduce_axis(0, 128)], \"C\") as [vi, vj, vk]:\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1649'>1650</a>\u001b[0m \u001b[39m                    C[vi, vj] = C[vi, vj] + A[vi, vk] * B[vj, vk]\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1650'>1651</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1651'>1652</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1652'>1653</a>\u001b[0m     block \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_normalize_block_arg(block)\n\u001b[1;32m   <a href='file:///Users/andrewzhaoluo/Desktop/dev_tvm/tvm/python/tvm/tir/schedule/schedule.py?line=1653'>1654</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _ffi_api\u001b[39m.\u001b[39mScheduleDecomposeReduction(\u001b[39mself\u001b[39m, block, loop)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tir' is not defined"
     ]
    }
   ],
   "source": [
    "sch = tvm.tir.Schedule(MyBmmRelu)\n",
    "print(\"original\")\n",
    "print(sch.mod.script())\n",
    "\n",
    "matmul_block = sch.get_block(\"matmul\", func_name=\"bmm_relu\")\n",
    "\n",
    "relu_block = sch.get_block(\"relu\", func_name=\"bmm_relu\")\n",
    "relu_n, relu_i, relu_j = sch.get_loops(relu_block)\n",
    "\n",
    "relu_j0, relu_j1 = sch.split(relu_j, [16, 8])\n",
    "\n",
    "sch.compute_at(matmul_block, relu_n)\n",
    "sch.compute_at(matmul_block, relu_i)\n",
    "sch.compute_at(matmul_block, relu_j0)\n",
    "\n",
    "sch.parallel(relu_n)\n",
    "sch.vectorize(relu_j1)\n",
    "\n",
    "matmul_n, matmul_i, matmul_j0, matmul_ax0, matmul_ax1 = sch.get_loops(matmul_block)\n",
    "init_block = sch.decompose_reduction(matmul_block, matmul_ax0)\n",
    "\n",
    "sch.reorder(matmul_ax1, matmul_ax0)\n",
    "ax1_0, ax1_1 = sch.split(matmul_ax1, [32, 4])\n",
    "sch.unroll(ax1_1)\n",
    "sch.vectorize(sch.get_loops(init_block)[3])\n",
    "\n",
    "\n",
    "print(\"now\")\n",
    "print(sch.mod.script())\n",
    "# print(\"target\")\n",
    "# print(tvm.tir.Schedule(TargetModule).mod.script())\n",
    "# tvm.ir.assert_structural_equal(sch.mod, TargetModule)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e7753e4d29c2cec5ef21324721b9a00fd282f5bdebbf8993c2dfe6634b1ed1d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tvm_py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
