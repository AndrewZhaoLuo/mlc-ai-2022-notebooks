{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import numpy as np\n",
    "import tvm\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm.script import tir as T\n",
    "from matplotlib import pyplot as plt \n",
    "import math "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5. Exercises for TensorIRÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.5.1: Exercise using j_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyModule:\n",
    "    @T.prim_func\n",
    "    def mm_relu(A: T.Buffer[(128, 128), \"float32\"],\n",
    "                B: T.Buffer[(128, 128), \"float32\"],\n",
    "                C: T.Buffer[(128, 128), \"float32\"]):\n",
    "        T.func_attr({\"global_symbol\": \"mm_relu\", \"tir.noalias\": True})\n",
    "        Y = T.alloc_buffer((128, 128), dtype=\"float32\")\n",
    "        for i, j, k in T.grid(128, 128, 128):\n",
    "            with T.block(\"Y\"):\n",
    "                vi = T.axis.spatial(128, i)\n",
    "                vj = T.axis.spatial(128, j)\n",
    "                vk = T.axis.reduce(128, k)\n",
    "                with T.init():\n",
    "                    Y[vi, vj] = T.float32(0)\n",
    "                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]\n",
    "        for i, j in T.grid(128, 128):\n",
    "            with T.block(\"C\"):\n",
    "                vi = T.axis.spatial(128, i)\n",
    "                vj = T.axis.spatial(128, j)\n",
    "                C[vi, vj] = T.max(Y[vi, vj], T.float32(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(mod, jfactor):\n",
    "    sch = tvm.tir.Schedule(mod)\n",
    "    block_Y = sch.get_block(\"Y\", func_name=\"mm_relu\")\n",
    "    i, j, k = sch.get_loops(block_Y)\n",
    "    j0, j1 = sch.split(j, factors=[None, jfactor])\n",
    "    sch.reorder(j0, k, j1)\n",
    "    block_C = sch.get_block(\"C\", \"mm_relu\")\n",
    "    sch.reverse_compute_at(block_C, j0)\n",
    "    return sch.mod\n",
    "\n",
    "def try_with_jfactor(jfactor: int):\n",
    "    dtype = \"float32\"\n",
    "    a_np = np.random.rand(128, 128).astype(dtype)\n",
    "    b_np = np.random.rand(128, 128).astype(dtype)\n",
    "    a_nd = tvm.nd.array(a_np)\n",
    "    b_nd = tvm.nd.array(b_np)\n",
    "    c_nd = tvm.nd.empty((128, 128), dtype=\"float32\")\n",
    "    type(c_nd)\n",
    "\n",
    "    mod_transformed = transform(MyModule, jfactor=jfactor)\n",
    "\n",
    "    rt_lib_transformed = tvm.build(mod_transformed, \"llvm\")\n",
    "    f_timer_transformed = rt_lib_transformed.time_evaluator(\"mm_relu\", tvm.cpu())\n",
    "    return f_timer_transformed(a_nd, b_nd, c_nd).mean, mod_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jfactor 1: 0.0041351483299999995 (s)\n",
      "jfactor 2: 0.00079739957 (s)\n",
      "jfactor 3: 0.00069842332 (s)\n",
      "jfactor 4: 0.00043632374 (s)\n",
      "jfactor 5: 0.00046278751000000006 (s)\n",
      "jfactor 6: 0.00038469499999999995 (s)\n",
      "jfactor 7: 0.0004572849899999999 (s)\n",
      "jfactor 8: 0.00026756834 (s)\n",
      "jfactor 9: 0.00045595999999999994 (s)\n",
      "jfactor 10: 0.00024293291 (s)\n",
      "jfactor 11: 0.00029557875 (s)\n",
      "jfactor 12: 0.00023001959000000004 (s)\n",
      "jfactor 13: 0.00021380124999999996 (s)\n",
      "jfactor 14: 0.00026665916 (s)\n",
      "jfactor 15: 0.00029555294 (s)\n",
      "jfactor 16: 0.00038959001 (s)\n",
      "jfactor 17: 0.00029445958 (s)\n",
      "jfactor 18: 0.00026651707 (s)\n",
      "jfactor 19: 0.00021114209 (s)\n",
      "jfactor 20: 0.00016973415999999998 (s)\n",
      "jfactor 21: 0.00046122000000000003 (s)\n",
      "jfactor 22: 0.00013955376999999998 (s)\n",
      "jfactor 23: 0.00047349082000000005 (s)\n",
      "jfactor 24: 0.00035822041999999997 (s)\n",
      "jfactor 25: 0.00085088127 (s)\n",
      "jfactor 26: 0.00107116625 (s)\n",
      "jfactor 27: 0.00076521833 (s)\n",
      "jfactor 28: 0.00125315668 (s)\n",
      "jfactor 29: 0.0011655641700000002 (s)\n",
      "jfactor 30: 0.0014007537700000002 (s)\n",
      "jfactor 31: 0.00117153418 (s)\n",
      "jfactor 32: 0.00036725791999999997 (s)\n",
      "jfactor 33: 0.0010638991699999998 (s)\n",
      "jfactor 34: 0.0011628229099999998 (s)\n",
      "jfactor 35: 0.0011504566700000001 (s)\n",
      "jfactor 36: 0.00120115415 (s)\n",
      "jfactor 37: 0.0012555475 (s)\n",
      "jfactor 38: 0.0013026495900000002 (s)\n",
      "jfactor 39: 0.0013508350099999998 (s)\n",
      "jfactor 40: 0.00139733958 (s)\n",
      "jfactor 41: 0.00115317668 (s)\n",
      "jfactor 42: 0.0014285153999999997 (s)\n",
      "jfactor 43: 0.00097333541 (s)\n",
      "jfactor 44: 0.0010454741399999999 (s)\n",
      "jfactor 45: 0.00107548457 (s)\n",
      "jfactor 46: 0.0011098795600000002 (s)\n",
      "jfactor 47: 0.00115266333 (s)\n",
      "jfactor 48: 0.00119273875 (s)\n",
      "jfactor 49: 0.00120707499 (s)\n",
      "jfactor 50: 0.0012433779300000001 (s)\n",
      "jfactor 51: 0.00127036667 (s)\n",
      "jfactor 52: 0.00131061459 (s)\n",
      "jfactor 53: 0.00135138458 (s)\n",
      "jfactor 54: 0.00146574168 (s)\n",
      "jfactor 55: 0.00149123498 (s)\n",
      "jfactor 56: 0.0014930470799999998 (s)\n",
      "jfactor 57: 0.00160445043 (s)\n",
      "jfactor 58: 0.00163289083 (s)\n",
      "jfactor 59: 0.0016084696 (s)\n",
      "jfactor 60: 0.00174128585 (s)\n",
      "jfactor 61: 0.0013458975100000001 (s)\n",
      "jfactor 62: 0.0018961891699999997 (s)\n",
      "jfactor 63: 0.00145955043 (s)\n",
      "jfactor 64: 0.00015311374999999998 (s)\n",
      "jfactor 65: 0.0009752771 (s)\n",
      "jfactor 66: 0.0013108096 (s)\n",
      "jfactor 67: 0.00124472834 (s)\n",
      "jfactor 68: 0.0012650341800000003 (s)\n",
      "jfactor 69: 0.0011248995700000003 (s)\n",
      "jfactor 70: 0.00116076624 (s)\n",
      "jfactor 71: 0.0012423133100000002 (s)\n",
      "jfactor 72: 0.00126646165 (s)\n",
      "jfactor 73: 0.0013090704099999997 (s)\n",
      "jfactor 74: 0.00122426626 (s)\n",
      "jfactor 75: 0.0012373029200000002 (s)\n",
      "jfactor 76: 0.0013420845800000002 (s)\n",
      "jfactor 77: 0.0013524591699999998 (s)\n",
      "jfactor 78: 0.0013801479300000002 (s)\n",
      "jfactor 79: 0.00132020667 (s)\n",
      "jfactor 80: 0.00134047121 (s)\n",
      "jfactor 81: 0.0014501541700000001 (s)\n",
      "jfactor 82: 0.00147120458 (s)\n",
      "jfactor 83: 0.00154592624 (s)\n",
      "jfactor 84: 0.00142805251 (s)\n",
      "jfactor 85: 0.0014666350000000002 (s)\n",
      "jfactor 86: 0.00156278043 (s)\n",
      "jfactor 87: 0.00157931251 (s)\n",
      "jfactor 88: 0.00159789334 (s)\n",
      "jfactor 89: 0.00158711249 (s)\n",
      "jfactor 90: 0.0016335137699999998 (s)\n",
      "jfactor 91: 0.00176378459 (s)\n",
      "jfactor 92: 0.0017451433399999997 (s)\n",
      "jfactor 93: 0.00177569835 (s)\n",
      "jfactor 94: 0.00178537208 (s)\n",
      "jfactor 95: 0.00174430875 (s)\n",
      "jfactor 96: 0.0017848662300000001 (s)\n",
      "jfactor 97: 0.0018152141699999999 (s)\n",
      "jfactor 98: 0.00192349668 (s)\n",
      "jfactor 99: 0.00195010373 (s)\n",
      "jfactor 100: 0.0019859491699999996 (s)\n",
      "jfactor 101: 0.00198506458 (s)\n",
      "jfactor 102: 0.00200549748 (s)\n",
      "jfactor 103: 0.00205045373 (s)\n",
      "jfactor 104: 0.00207473959 (s)\n",
      "jfactor 105: 0.00207908749 (s)\n",
      "jfactor 106: 0.00210211248 (s)\n",
      "jfactor 107: 0.00213261458 (s)\n",
      "jfactor 108: 0.0021367641500000005 (s)\n",
      "jfactor 109: 0.00222722292 (s)\n",
      "jfactor 110: 0.0021977916800000005 (s)\n",
      "jfactor 111: 0.0023475483199999997 (s)\n",
      "jfactor 112: 0.0022426912500000005 (s)\n",
      "jfactor 113: 0.00228036834 (s)\n",
      "jfactor 114: 0.00230500499 (s)\n",
      "jfactor 115: 0.00229695875 (s)\n",
      "jfactor 116: 0.002356501650000001 (s)\n",
      "jfactor 117: 0.00234867291 (s)\n",
      "jfactor 118: 0.0023817108100000002 (s)\n",
      "jfactor 119: 0.0024175595700000004 (s)\n",
      "jfactor 120: 0.00242221167 (s)\n",
      "jfactor 121: 0.00174221793 (s)\n",
      "jfactor 122: 0.0024804954000000002 (s)\n",
      "jfactor 123: 0.00177137541 (s)\n",
      "jfactor 124: 0.00250374168 (s)\n",
      "jfactor 125: 0.0018191778999999998 (s)\n",
      "jfactor 126: 0.0024949420899999996 (s)\n",
      "jfactor 127: 0.00185657459 (s)\n",
      "jfactor 128: 0.00014996747999999996 (s)\n",
      "[(-8.877060582940189, 22), (-8.805092087372536, 128), (-8.784329448752045, 64), (-8.681277109576076, 20), (-8.462979238806268, 19), (-8.450463712804826, 13), (-8.377346078755126, 12), (-8.322725243278713, 10), (-8.230072263876817, 18), (-8.229539269360675, 14), (-8.226135547621297, 8), (-8.130368813867769, 17), (-8.126662583187967, 15), (-8.126575259160333, 11), (-7.934362062704, 24), (-7.909446177451877, 32), (-7.8630597454311175, 6), (-7.850415628208426, 16), (-7.7371260672132305, 4), (-7.693105471595697, 9), (-7.690203750931016, 7), (-7.681635405375179, 21), (-7.678242550908673, 5), (-7.655378033065681, 23), (-7.2666851633892335, 3), (-7.175349366163387, 27), (-7.134154662269736, 2), (-7.069237957338338, 25), (-6.934781817810689, 43), (-6.932788922216849, 65), (-6.863284773978248, 44), (-6.845814657590768, 33), (-6.839007270805093, 26), (-6.834983956174841, 45), (-6.803503774049438, 46), (-6.790061518421767, 69), (-6.767596311084216, 35), (-6.76568007513018, 47), (-6.765234814395476, 41), (-6.758674940207991, 70), (-6.7569046870312635, 34), (-6.7545500430890355, 29), (-6.749441124172304, 31), (-6.731503145597448, 48), (-6.724472392746768, 36), (-6.719555209550619, 49), (-6.705413585869067, 74), (-6.6948213327697585, 75), (-6.690780064803808, 71), (-6.689923466005078, 50), (-6.688837973677938, 67), (-6.682089566990889, 28), (-6.680183546549848, 37), (-6.672656137404835, 68), (-6.671528369281042, 72), (-6.668449703646803, 51), (-6.643354942562799, 38), (-6.638438004348934, 73), (-6.6372590991150275, 52), (-6.637110317394691, 66), (-6.629966986457558, 79), (-6.614734077579694, 80), (-6.61353121708334, 76), (-6.61069419478947, 61), (-6.607032351814193, 39), (-6.606625597300525, 53), (-6.605830736239062, 77), (-6.585564589905893, 78), (-6.573185150221651, 40), (-6.570744780107189, 30), (-6.5514436440321235, 84), (-6.551119555812529, 42), (-6.5360854040637335, 81), (-6.529626815337958, 63), (-6.5253938984110045, 54), (-6.524784617868209, 85), (-6.5216737715740605, 82), (-6.508150656690047, 55), (-6.50693622709775, 56), (-6.472132040174998, 83), (-6.461288717257465, 86), (-6.450765646888713, 87), (-6.445838957782275, 89), (-6.439069179798079, 88), (-6.434973992251217, 57), (-6.432472111059689, 59), (-6.417403319647392, 58), (-6.417021897206613, 90), (-6.353131444424629, 60), (-6.352596305047088, 121), (-6.3513969336138345, 95), (-6.3509185834348445, 92), (-6.340293443366593, 91), (-6.335998966360654, 123), (-6.333561496810932, 93), (-6.328411807735235, 96), (-6.328128437293566, 94), (-6.311551818226776, 97), (-6.3093705832388025, 125), (-6.289022107400406, 127), (-6.267909106919571, 62), (-6.253610561793689, 98), (-6.2398727129494835, 99), (-6.222103831367165, 101), (-6.221658307845806, 100), (-6.211863129299458, 102), (-6.189694178614752, 103), (-6.177919631970289, 104), (-6.175826188259632, 105), (-6.164812497017077, 106), (-6.150406549678441, 107), (-6.148462673733451, 108), (-6.120302204562401, 110), (-6.106999796928849, 109), (-6.100078683386884, 112), (-6.08341829643243, 113), (-6.076169313642782, 115), (-6.072672437676866, 114), (-6.054383763494122, 111), (-6.05390482948892, 117), (-6.050577111409772, 116), (-6.0399362218005725, 118), (-6.024996689782191, 119), (-6.023074242954821, 120), (-5.999296980689715, 122), (-5.993489760468863, 126), (-5.989968994005244, 124), (-5.488232079323985, 1)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_benchmarks = []\n",
    "jfactors = []\n",
    "\n",
    "for jfactor in range(1, 129):\n",
    "    times = []\n",
    "    for repeats in range(10):\n",
    "        time, mod_transformed = try_with_jfactor(jfactor)\n",
    "        times.append(time)\n",
    "    mean_benchmark = sum(times) / len(times)\n",
    "    print(f\"jfactor {jfactor}: {mean_benchmark} (s)\")\n",
    "    mean_benchmarks.append(mean_benchmark)\n",
    "    jfactors.append(jfactor)\n",
    "\n",
    "log_mean_benchmarks = [math.log(d) for d in mean_benchmarks]\n",
    "plt.scatter(jfactors, log_mean_benchmarks)\n",
    "\n",
    "log_mean_benchmarks = [(log_mean_benchmarks[i], i + 1) for i in range(len(log_mean_benchmarks))]\n",
    "log_mean_benchmarks = sorted(log_mean_benchmarks)\n",
    "print(log_mean_benchmarks)\n",
    "\n",
    "# 22 jfactor, 128, and 64 are the fastest! Weird that 22 is so fast...\n",
    "# Also shape of graph is weird... perhaps related to some kind of codegen thing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5.1.1. Example: Element-wise Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init data\n",
    "a = np.arange(16).reshape(4, 4)\n",
    "b = np.arange(16, 0, -1).reshape(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy version\n",
    "c_np = a + b\n",
    "c_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# low-level numpy version\n",
    "def lnumpy_add(a: np.ndarray, b: np.ndarray, c: np.ndarray):\n",
    "  for i in range(4):\n",
    "    for j in range(4):\n",
    "      c[i, j] = a[i, j] + b[i, j]\n",
    "c_lnumpy = np.empty((4, 4), dtype=np.int64)\n",
    "lnumpy_add(a, b, c_lnumpy)\n",
    "c_lnumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorIR version\n",
    "@tvm.script.ir_module\n",
    "class MyAdd:\n",
    "  @T.prim_func\n",
    "  def add(A: T.Buffer[(4, 4), \"int64\"],\n",
    "          B: T.Buffer[(4, 4), \"int64\"],\n",
    "          C: T.Buffer[(4, 4), \"int64\"]):\n",
    "    T.func_attr({\"global_symbol\": \"add\"})\n",
    "    for i, j in T.grid(4, 4):\n",
    "      with T.block(\"C\"):\n",
    "        vi = T.axis.spatial(4, i)\n",
    "        vj = T.axis.spatial(4, j)\n",
    "        C[vi, vj] = A[vi, vj] + B[vi, vj]\n",
    "\n",
    "rt_lib = tvm.build(MyAdd, target=\"llvm\")\n",
    "a_tvm = tvm.nd.array(a)\n",
    "b_tvm = tvm.nd.array(b)\n",
    "c_tvm = tvm.nd.array(np.empty((4, 4), dtype=np.int64))\n",
    "rt_lib[\"add\"](a_tvm, b_tvm, c_tvm)\n",
    "np.testing.assert_allclose(c_tvm.numpy(), c_np, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5.1.2. Exercise 1: Broadcast Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init data\n",
    "a = np.arange(16).reshape(4, 4)\n",
    "b = np.arange(4, 0, -1).reshape(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  4,  4,  4],\n",
       "       [ 8,  8,  8,  8],\n",
       "       [12, 12, 12, 12],\n",
       "       [16, 16, 16, 16]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy version\n",
    "c_np = a + b\n",
    "c_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyAdd:\n",
    "  @T.prim_func\n",
    "  def add(\n",
    "      A: T.Buffer[(4, 4), \"int64\"], \n",
    "      B: T.Buffer[(4), \"int64\"],\n",
    "      C: T.Buffer[(4, 4), \"int64\"]\n",
    "    ):\n",
    "        T.func_attr({\"global_symbol\": \"add\", \"tir.noalias\": True})\n",
    "        for i, j in T.grid(4, 4):\n",
    "            with T.block(\"C\"):\n",
    "                vi = T.axis.spatial(4, i)\n",
    "                vj = T.axis.spatial(4, j)\n",
    "                C[vi, vj] = A[vi, vj] + B[vj]\n",
    "\n",
    "rt_lib = tvm.build(MyAdd, target=\"llvm\")\n",
    "a_tvm = tvm.nd.array(a)\n",
    "b_tvm = tvm.nd.array(b)\n",
    "c_tvm = tvm.nd.array(np.empty((4, 4), dtype=np.int64))\n",
    "rt_lib[\"add\"](a_tvm, b_tvm, c_tvm)\n",
    "np.testing.assert_allclose(c_tvm.numpy(), c_np, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5.1.3. Exercise 2: 2D ConvolutionÂ¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, CI, H, W, CO, K = 1, 1, 8, 8, 2, 3\n",
    "OUT_H, OUT_W = H - K + 1, W - K + 1\n",
    "data = np.arange(N*CI*H*W).reshape(N, CI, H, W)\n",
    "weight = np.arange(CO*CI*K*K).reshape(CO, CI, K, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[                474,                 510,\n",
       "                          546,                 582,\n",
       "                          618,                 654],\n",
       "         [                762,                 798,\n",
       "                          834,                 870,\n",
       "                          906,                 942],\n",
       "         [               1050,                1086,\n",
       "                         1122,                1158,\n",
       "                         1194,                1230],\n",
       "         [               1338,                1374,\n",
       "                         1410,                1446,\n",
       "                         1482,                1518],\n",
       "         [               1626,                1662,\n",
       "                         1698,                1734,\n",
       "                         1770,                1806],\n",
       "         [               1914,                1950,\n",
       "                         1986,                2022,\n",
       "                         2058,                2094]],\n",
       "\n",
       "        [[                  0, 9223372036854775807,\n",
       "                            0, 9223372036854775807,\n",
       "          9223372036854775807,              656070],\n",
       "         [9223372036854775807,        267770429440,\n",
       "                            0, 9223372036854775807,\n",
       "          9223372036854775807,                   0],\n",
       "         [                  0, 9223372036854775807,\n",
       "                            0, 9223372036854775807,\n",
       "                            0, 9223372036854775807],\n",
       "         [                  0, 9223372036854775807,\n",
       "          9223372036854775807, 9223372036854775807,\n",
       "                            0, 9223372036854775807],\n",
       "         [9223372036854775807,                   0,\n",
       "                            0,                   0,\n",
       "          9223372036854775807,                   0],\n",
       "         [                  0,                   0,\n",
       "                            0, 9223372036854775807,\n",
       "                            0, 9223372036854775807]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch version\n",
    "import torch\n",
    "\n",
    "data_torch = torch.Tensor(data)\n",
    "weight_torch = torch.Tensor(weight)\n",
    "conv_torch = torch.nn.functional.conv2d(data_torch, weight_torch)\n",
    "conv_torch = conv_torch.numpy().astype(np.int64)\n",
    "conv_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyConv:\n",
    "  N, CI, H, W, CO, K = 1, 1, 8, 8, 2, 3\n",
    "  OUT_H, OUT_W = H - K + 1, W - K + 1\n",
    "  @T.prim_func\n",
    "  def conv(\n",
    "    in_img: T.Buffer[(1, 1, 8, 8), \"int64\"],\n",
    "    filters: T.Buffer[(2, 1, 3, 3), \"int64\"],\n",
    "    out_img: T.Buffer[(1, 2, 6, 6), \"int64\"]\n",
    "  ):\n",
    "    T.func_attr({\"global_symbol\": \"conv\", \"tir.noalias\": True})\n",
    "    for n, co, i, j, kh, kw in T.grid(1, 2, 6, 6, 3, 3):\n",
    "      with T.block(\"conv\"):\n",
    "        vn, vco, vi, vj, vkh, vkw = T.axis.remap(\"SSSSRR\", [n, co, i, j, kh, kw]) \n",
    "        with T.init():\n",
    "          out_img[vn, vco, vi, vj] = T.int64(0)\n",
    "        out_img[vn, vco, vi, vj] += in_img[0, 0, vi + vkh, vj + vkw] * filters[vco, 0, vkh, vkw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nNot equal to tolerance rtol=1e-05, atol=0\n\nMismatched elements: 36 / 72 (50%)\nMax absolute difference: 9223372036854774487\nMax relative difference: 1.\n x: array([[[[ 474,  510,  546,  582,  618,  654],\n         [ 762,  798,  834,  870,  906,  942],\n         [1050, 1086, 1122, 1158, 1194, 1230],...\n y: array([[[[                474,                 510,\n                          546,                 582,\n                          618,                 654],...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb#ch0000015?line=3'>4</a>\u001b[0m conv_tvm \u001b[39m=\u001b[39m tvm\u001b[39m.\u001b[39mnd\u001b[39m.\u001b[39marray(np\u001b[39m.\u001b[39mempty((N, CO, OUT_H, OUT_W), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint64))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb#ch0000015?line=4'>5</a>\u001b[0m rt_lib[\u001b[39m\"\u001b[39m\u001b[39mconv\u001b[39m\u001b[39m\"\u001b[39m](data_tvm, weight_tvm, conv_tvm)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/andrewzhaoluo/Desktop/dev_tvm/mlc-ai-2022-notebooks/exercises/exercises_2.5.ipynb#ch0000015?line=5'>6</a>\u001b[0m np\u001b[39m.\u001b[39;49mtesting\u001b[39m.\u001b[39;49massert_allclose(conv_tvm\u001b[39m.\u001b[39;49mnumpy(), conv_torch, rtol\u001b[39m=\u001b[39;49m\u001b[39m1e-5\u001b[39;49m)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/tvm_py3.8/lib/python3.8/site-packages/numpy/testing/_private/utils.py:844\u001b[0m, in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/miniforge3/envs/tvm_py3.8/lib/python3.8/site-packages/numpy/testing/_private/utils.py?line=839'>840</a>\u001b[0m         err_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(remarks)\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/miniforge3/envs/tvm_py3.8/lib/python3.8/site-packages/numpy/testing/_private/utils.py?line=840'>841</a>\u001b[0m         msg \u001b[39m=\u001b[39m build_err_msg([ox, oy], err_msg,\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/miniforge3/envs/tvm_py3.8/lib/python3.8/site-packages/numpy/testing/_private/utils.py?line=841'>842</a>\u001b[0m                             verbose\u001b[39m=\u001b[39mverbose, header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/miniforge3/envs/tvm_py3.8/lib/python3.8/site-packages/numpy/testing/_private/utils.py?line=842'>843</a>\u001b[0m                             names\u001b[39m=\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m), precision\u001b[39m=\u001b[39mprecision)\n\u001b[0;32m--> <a href='file:///Users/andrewzhaoluo/miniforge3/envs/tvm_py3.8/lib/python3.8/site-packages/numpy/testing/_private/utils.py?line=843'>844</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(msg)\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/miniforge3/envs/tvm_py3.8/lib/python3.8/site-packages/numpy/testing/_private/utils.py?line=844'>845</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/andrewzhaoluo/miniforge3/envs/tvm_py3.8/lib/python3.8/site-packages/numpy/testing/_private/utils.py?line=845'>846</a>\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtraceback\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=1e-05, atol=0\n\nMismatched elements: 36 / 72 (50%)\nMax absolute difference: 9223372036854774487\nMax relative difference: 1.\n x: array([[[[ 474,  510,  546,  582,  618,  654],\n         [ 762,  798,  834,  870,  906,  942],\n         [1050, 1086, 1122, 1158, 1194, 1230],...\n y: array([[[[                474,                 510,\n                          546,                 582,\n                          618,                 654],..."
     ]
    }
   ],
   "source": [
    "rt_lib = tvm.build(MyConv, target=\"llvm\")\n",
    "data_tvm = tvm.nd.array(data)\n",
    "weight_tvm = tvm.nd.array(weight)\n",
    "conv_tvm = tvm.nd.array(np.empty((N, CO, OUT_H, OUT_W), dtype=np.int64))\n",
    "rt_lib[\"conv\"](data_tvm, weight_tvm, conv_tvm)\n",
    "np.testing.assert_allclose(conv_tvm.numpy(), conv_torch, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5.2.2 Batch Matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnumpy_mm_relu_v2(A: np.ndarray, B: np.ndarray, C: np.ndarray):\n",
    "    Y = np.empty((16, 128, 128), dtype=\"float32\")\n",
    "    for n in range(16):\n",
    "        for i in range(128):\n",
    "            for j in range(128):\n",
    "                for k in range(128):\n",
    "                    if k == 0:\n",
    "                        Y[n, i, j] = 0\n",
    "                    Y[n, i, j] = Y[n, i, j] + A[n, i, k] * B[n, k, j]\n",
    "    for n in range(16):\n",
    "        for i in range(128):\n",
    "            for j in range(128):\n",
    "                C[n, i, j] = max(Y[n, i, j], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# from tvm.script import tir as T\n",
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def bmm_relu(A: T.Buffer[(16, 128, 128), \"float32\"], B: T.Buffer[(16, 128, 128), \"float32\"], C: T.Buffer[(16, 128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with T.block(\"root\")\n",
      "        intermediate = T.alloc_buffer([16, 128, 128], dtype=\"float32\")\n",
      "        for n, i, j, k in T.grid(16, 128, 128, 128):\n",
      "            with T.block(\"matmul\"):\n",
      "                vn, vi, vj, vk = T.axis.remap(\"SSSR\", [n, i, j, k])\n",
      "                T.reads(A[vn, vi, vk], B[vn, vk, vj])\n",
      "                T.writes(intermediate[vn, vi, vj])\n",
      "                with T.init():\n",
      "                    intermediate[vn, vi, vj] = T.float32(0)\n",
      "                intermediate[vn, vi, vj] = intermediate[vn, vi, vj] + A[vn, vi, vk] * B[vn, vk, vj]\n",
      "        for n, i, j in T.grid(16, 128, 128):\n",
      "            with T.block(\"relu\"):\n",
      "                vn, vi, vj = T.axis.remap(\"SSS\", [n, i, j])\n",
      "                T.reads(intermediate[vn, vi, vj])\n",
      "                T.writes(C[vn, vi, vj])\n",
      "                C[vn, vi, vj] = T.max(intermediate[vn, vi, vj], T.float32(0))\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyBmmRelu:\n",
    "  @T.prim_func\n",
    "  def bmm_relu(\n",
    "      A: T.Buffer[(16, 128, 128), \"float32\"],\n",
    "      B: T.Buffer[(16, 128, 128), \"float32\"],\n",
    "      C: T.Buffer[(16, 128, 128), \"float32\"]\n",
    "  ):\n",
    "    T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
    "\n",
    "    intermediate = T.alloc_buffer((16, 128, 128), \"float32\")\n",
    "    for n, i, j, k in T.grid(16, 128, 128, 128):\n",
    "      with T.block(\"matmul\"):\n",
    "        vn, vi, vj, vk = T.axis.remap(\"SSSR\", [n, i, j, k])\n",
    "        with T.init(): \n",
    "          intermediate[vn, vi, vj] = T.float32(0)\n",
    "        intermediate[vn, vi, vj] += A[vn, vi, vk] * B[vn, vk, vj]\n",
    "    for n, i, j in T.grid(16, 128, 128):\n",
    "      with T.block(\"relu\"):\n",
    "        vn, vi, vj = T.axis.remap(\"SSS\", [n, i, j])\n",
    "        C[vn, vi, vj] = T.max(intermediate[vn, vi, vj], T.float32(0))\n",
    "\n",
    "print(MyBmmRelu.script())\n",
    "#sch = tvm.tir.Schedule(MyBmmRelu)\n",
    "#IPython.display.Code(sch.mod.script(), language=\"python\")\n",
    "# Also please validate your result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# from tvm.script import tir as T\n",
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def bmm_relu(A: T.Buffer[(16, 128, 128), \"float32\"], B: T.Buffer[(16, 128, 128), \"float32\"], C: T.Buffer[(16, 128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with T.block(\"root\")\n",
      "        intermediate = T.alloc_buffer([16, 128, 128], dtype=\"float32\")\n",
      "        for n in T.parallel(16):\n",
      "            for ax0, ax1, ax2 in T.grid(128, 128, 128):\n",
      "                with T.block(\"matmul\"):\n",
      "                    vn, vi, vj, vk = T.axis.remap(\"SSSR\", [n, ax0, ax1, ax2])\n",
      "                    T.reads(A[vn, vi, vk], B[vn, vk, vj])\n",
      "                    T.writes(intermediate[vn, vi, vj])\n",
      "                    with T.init():\n",
      "                        intermediate[vn, vi, vj] = T.float32(0)\n",
      "                    intermediate[vn, vi, vj] = intermediate[vn, vi, vj] + A[vn, vi, vk] * B[vn, vk, vj]\n",
      "            for i, j in T.grid(128, 128):\n",
      "                with T.block(\"relu\"):\n",
      "                    vn, vi, vj = T.axis.remap(\"SSS\", [n, i, j])\n",
      "                    T.reads(intermediate[vn, vi, vj])\n",
      "                    T.writes(C[vn, vi, vj])\n",
      "                    C[vn, vi, vj] = T.max(intermediate[vn, vi, vj], T.float32(0))\n",
      "    \n",
      "\n",
      "**************************************************\n",
      "# from tvm.script import tir as T\n",
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def bmm_relu(A: T.Buffer[(16, 128, 128), \"float32\"], B: T.Buffer[(16, 128, 128), \"float32\"], C: T.Buffer[(16, 128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with T.block(\"root\")\n",
      "        intermediate = T.alloc_buffer([16, 128, 128], dtype=\"float32\")\n",
      "        for n_init in T.parallel(16):\n",
      "            for ax0_init, ax1_init in T.grid(128, 128):\n",
      "                with T.block(\"matmul_init\"):\n",
      "                    vn, vi, vj = T.axis.remap(\"SSS\", [n_init, ax0_init, ax1_init])\n",
      "                    T.reads()\n",
      "                    T.writes(intermediate[vn, vi, vj])\n",
      "                    intermediate[vn, vi, vj] = T.float32(0)\n",
      "        for n in T.parallel(16):\n",
      "            for ax0, ax1, ax2 in T.grid(128, 128, 128):\n",
      "                with T.block(\"matmul_update\"):\n",
      "                    vn, vi, vj, vk = T.axis.remap(\"SSSR\", [n, ax0, ax1, ax2])\n",
      "                    T.reads(intermediate[vn, vi, vj], A[vn, vi, vk], B[vn, vk, vj])\n",
      "                    T.writes(intermediate[vn, vi, vj])\n",
      "                    intermediate[vn, vi, vj] = intermediate[vn, vi, vj] + A[vn, vi, vk] * B[vn, vk, vj]\n",
      "            for i, j in T.grid(128, 128):\n",
      "                with T.block(\"relu\"):\n",
      "                    vn, vi, vj = T.axis.remap(\"SSS\", [n, i, j])\n",
      "                    T.reads(intermediate[vn, vi, vj])\n",
      "                    T.writes(C[vn, vi, vj])\n",
      "                    C[vn, vi, vj] = T.max(intermediate[vn, vi, vj], T.float32(0))\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sch = tvm.tir.Schedule(MyBmmRelu)\n",
    "\n",
    "matmul_block = sch.get_block(\"matmul\", func_name=\"bmm_relu\")\n",
    "matmul_n, matmul_i, matmul_j, matmul_k = sch.get_loops(matmul_block)\n",
    "\n",
    "#k0, k1 = sch.split(matmul_k, [8, 16])\n",
    "# sch.reorder(k1, k0)\n",
    "\n",
    "res = sch.get_loops(sch.get_block(\"relu\", func_name=\"bmm_relu\"))\n",
    "sch.compute_at(matmul_block, res[0])\n",
    "sch.parallel(res[0])\n",
    "\n",
    "matmul_n, matmul_i, matmul_j, matmul_k = sch.get_loops(matmul_block)\n",
    "\n",
    "print(sch.mod.script())\n",
    "print('*' * 50)\n",
    "sch.decompose_reduction(matmul_block, matmul_n)\n",
    "\n",
    "'''\n",
    "# TODO: transformations\n",
    "# Hints: you can use\n",
    "# `IPython.display.Code(sch.mod.script(), language=\"python\")`\n",
    "# or `print(sch.mod.script())`\n",
    "# to show the current program at any time during the transformation.\n",
    "\n",
    "# Step 1. Get blocks\n",
    "Y = sch.get_block(\"Y\", func_name=\"bmm_relu\")\n",
    "...\n",
    "\n",
    "# Step 2. Get loops\n",
    "b, i, j, k = sch.get_loops(Y)\n",
    "...\n",
    "\n",
    "# Step 3. Organize the loops\n",
    "k0, k1 = sch.split(k, ...)\n",
    "sch.reorder(...)\n",
    "sch.compute_at/reverse_compute_at(...)\n",
    "...\n",
    "\n",
    "# Step 4. decompose reduction\n",
    "Y_init = sch.decompose_reduction(Y, ...)\n",
    "...\n",
    "\n",
    "# Step 5. vectorize / parallel / unroll\n",
    "sch.vectorize(...)\n",
    "sch.parallel(...)\n",
    "sch.unroll(...)\n",
    "...\n",
    "\n",
    "IPython.display.Code(sch.mod.script(), language=\"python\")\n",
    "'''\n",
    "\n",
    "print(sch.mod.script())\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e7753e4d29c2cec5ef21324721b9a00fd282f5bdebbf8993c2dfe6634b1ed1d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tvm_py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
