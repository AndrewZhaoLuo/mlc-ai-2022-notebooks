{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5. Exercises for TensorIR¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import numpy as np\n",
    "import tvm\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm.script import tir as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5.1.1. Example: Element-wise Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init data\n",
    "a = np.arange(16).reshape(4, 4)\n",
    "b = np.arange(16, 0, -1).reshape(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy version\n",
    "c_np = a + b\n",
    "c_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# low-level numpy version\n",
    "def lnumpy_add(a: np.ndarray, b: np.ndarray, c: np.ndarray):\n",
    "  for i in range(4):\n",
    "    for j in range(4):\n",
    "      c[i, j] = a[i, j] + b[i, j]\n",
    "c_lnumpy = np.empty((4, 4), dtype=np.int64)\n",
    "lnumpy_add(a, b, c_lnumpy)\n",
    "c_lnumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorIR version\n",
    "@tvm.script.ir_module\n",
    "class MyAdd:\n",
    "  @T.prim_func\n",
    "  def add(A: T.Buffer[(4, 4), \"int64\"],\n",
    "          B: T.Buffer[(4, 4), \"int64\"],\n",
    "          C: T.Buffer[(4, 4), \"int64\"]):\n",
    "    T.func_attr({\"global_symbol\": \"add\"})\n",
    "    for i, j in T.grid(4, 4):\n",
    "      with T.block(\"C\"):\n",
    "        vi = T.axis.spatial(4, i)\n",
    "        vj = T.axis.spatial(4, j)\n",
    "        C[vi, vj] = A[vi, vj] + B[vi, vj]\n",
    "\n",
    "rt_lib = tvm.build(MyAdd, target=\"llvm\")\n",
    "a_tvm = tvm.nd.array(a)\n",
    "b_tvm = tvm.nd.array(b)\n",
    "c_tvm = tvm.nd.array(np.empty((4, 4), dtype=np.int64))\n",
    "rt_lib[\"add\"](a_tvm, b_tvm, c_tvm)\n",
    "np.testing.assert_allclose(c_tvm.numpy(), c_np, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5.1.2. Exercise 1: Broadcast Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init data\n",
    "a = np.arange(16).reshape(4, 4)\n",
    "b = np.arange(4, 0, -1).reshape(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  4,  4,  4],\n",
       "       [ 8,  8,  8,  8],\n",
       "       [12, 12, 12, 12],\n",
       "       [16, 16, 16, 16]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy version\n",
    "c_np = a + b\n",
    "c_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyAdd:\n",
    "  @T.prim_func\n",
    "  def add(\n",
    "      A: T.Buffer[(4, 4), \"int64\"], \n",
    "      B: T.Buffer[(4), \"int64\"],\n",
    "      C: T.Buffer[(4, 4), \"int64\"]\n",
    "    ):\n",
    "        T.func_attr({\"global_symbol\": \"add\", \"tir.noalias\": True})\n",
    "        for i, j in T.grid(4, 4):\n",
    "            with T.block(\"C\"):\n",
    "                vi = T.axis.spatial(4, i)\n",
    "                vj = T.axis.spatial(4, j)\n",
    "                C[vi, vj] = A[vi, vj] + B[vj]\n",
    "\n",
    "rt_lib = tvm.build(MyAdd, target=\"llvm\")\n",
    "a_tvm = tvm.nd.array(a)\n",
    "b_tvm = tvm.nd.array(b)\n",
    "c_tvm = tvm.nd.array(np.empty((4, 4), dtype=np.int64))\n",
    "rt_lib[\"add\"](a_tvm, b_tvm, c_tvm)\n",
    "np.testing.assert_allclose(c_tvm.numpy(), c_np, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5.1.3. Exercise 2: 2D Convolution¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, CI, H, W, CO, K = 1, 1, 8, 8, 2, 3\n",
    "OUT_H, OUT_W = H - K + 1, W - K + 1\n",
    "data = np.arange(N*CI*H*W).reshape(N, CI, H, W)\n",
    "weight = np.arange(CO*CI*K*K).reshape(CO, CI, K, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 474,  510,  546,  582,  618,  654],\n",
       "         [ 762,  798,  834,  870,  906,  942],\n",
       "         [1050, 1086, 1122, 1158, 1194, 1230],\n",
       "         [1338, 1374, 1410, 1446, 1482, 1518],\n",
       "         [1626, 1662, 1698, 1734, 1770, 1806],\n",
       "         [1914, 1950, 1986, 2022, 2058, 2094]],\n",
       "\n",
       "        [[1203, 1320, 1437, 1554, 1671, 1788],\n",
       "         [2139, 2256, 2373, 2490, 2607, 2724],\n",
       "         [3075, 3192, 3309, 3426, 3543, 3660],\n",
       "         [4011, 4128, 4245, 4362, 4479, 4596],\n",
       "         [4947, 5064, 5181, 5298, 5415, 5532],\n",
       "         [5883, 6000, 6117, 6234, 6351, 6468]]]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch version\n",
    "import torch\n",
    "\n",
    "data_torch = torch.Tensor(data)\n",
    "weight_torch = torch.Tensor(weight)\n",
    "conv_torch = torch.nn.functional.conv2d(data_torch, weight_torch)\n",
    "conv_torch = conv_torch.numpy().astype(np.int64)\n",
    "conv_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyConv:\n",
    "  N, CI, H, W, CO, K = 1, 1, 8, 8, 2, 3\n",
    "  OUT_H, OUT_W = H - K + 1, W - K + 1\n",
    "  @T.prim_func\n",
    "  def conv(\n",
    "    in_img: T.Buffer[(1, 1, 8, 8), \"int64\"],\n",
    "    filters: T.Buffer[(2, 1, 3, 3), \"int64\"],\n",
    "    out_img: T.Buffer[(1, 2, 6, 6), \"int64\"]\n",
    "  ):\n",
    "    T.func_attr({\"global_symbol\": \"conv\", \"tir.noalias\": True})\n",
    "    for n, co, i, j, kh, kw in T.grid(1, 2, 6, 6, 3, 3):\n",
    "      with T.block(\"conv\"):\n",
    "        vn, vco, vi, vj, vkh, vkw = T.axis.remap(\"SSSSRR\", [n, co, i, j, kh, kw]) \n",
    "        with T.init():\n",
    "          out_img[vn, vco, vi, vj] = T.int64(0)\n",
    "        out_img[vn, vco, vi, vj] += in_img[0, 0, vi + vkh, vj + vkw] * filters[vco, 0, vkh, vkw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_lib = tvm.build(MyConv, target=\"llvm\")\n",
    "data_tvm = tvm.nd.array(data)\n",
    "weight_tvm = tvm.nd.array(weight)\n",
    "conv_tvm = tvm.nd.array(np.empty((N, CO, OUT_H, OUT_W), dtype=np.int64))\n",
    "rt_lib[\"conv\"](data_tvm, weight_tvm, conv_tvm)\n",
    "np.testing.assert_allclose(conv_tvm.numpy(), conv_torch, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5.2.2 Batch Matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnumpy_mm_relu_v2(A: np.ndarray, B: np.ndarray, C: np.ndarray):\n",
    "    Y = np.empty((16, 128, 128), dtype=\"float32\")\n",
    "    for n in range(16):\n",
    "        for i in range(128):\n",
    "            for j in range(128):\n",
    "                for k in range(128):\n",
    "                    if k == 0:\n",
    "                        Y[n, i, j] = 0\n",
    "                    Y[n, i, j] = Y[n, i, j] + A[n, i, k] * B[n, k, j]\n",
    "    for n in range(16):\n",
    "        for i in range(128):\n",
    "            for j in range(128):\n",
    "                C[n, i, j] = max(Y[n, i, j], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# from tvm.script import tir as T\n",
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def bmm_relu(A: T.Buffer[(16, 128, 128), \"float32\"], B: T.Buffer[(16, 128, 128), \"float32\"], C: T.Buffer[(16, 128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with T.block(\"root\")\n",
      "        intermediate = T.alloc_buffer([16, 128, 128], dtype=\"float32\")\n",
      "        for n, i, j, k in T.grid(16, 128, 128, 128):\n",
      "            with T.block(\"matmul\"):\n",
      "                vn, vi, vj, vk = T.axis.remap(\"SSSR\", [n, i, j, k])\n",
      "                T.reads(A[vn, vi, vk], B[vn, vk, vj])\n",
      "                T.writes(intermediate[vn, vi, vj])\n",
      "                with T.init():\n",
      "                    intermediate[vn, vi, vj] = T.float32(0)\n",
      "                intermediate[vn, vi, vj] = intermediate[vn, vi, vj] + A[vn, vi, vk] * B[vn, vk, vj]\n",
      "        for n, i, j in T.grid(16, 128, 128):\n",
      "            with T.block(\"relu\"):\n",
      "                vn, vi, vj = T.axis.remap(\"SSS\", [n, i, j])\n",
      "                T.reads(intermediate[vn, vi, vj])\n",
      "                T.writes(C[vn, vi, vj])\n",
      "                C[vn, vi, vj] = T.max(intermediate[vn, vi, vj], T.float32(0))\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyBmmRelu:\n",
    "  @T.prim_func\n",
    "  def bmm_relu(\n",
    "      A: T.Buffer[(16, 128, 128), \"float32\"],\n",
    "      B: T.Buffer[(16, 128, 128), \"float32\"],\n",
    "      C: T.Buffer[(16, 128, 128), \"float32\"]\n",
    "  ):\n",
    "    T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
    "\n",
    "    intermediate = T.alloc_buffer((16, 128, 128), \"float32\")\n",
    "    for n, i, j, k in T.grid(16, 128, 128, 128):\n",
    "      with T.block(\"matmul\"):\n",
    "        vn, vi, vj, vk = T.axis.remap(\"SSSR\", [n, i, j, k])\n",
    "        with T.init(): \n",
    "          intermediate[vn, vi, vj] = T.float32(0)\n",
    "        intermediate[vn, vi, vj] += A[vn, vi, vk] * B[vn, vk, vj]\n",
    "    for n, i, j in T.grid(16, 128, 128):\n",
    "      with T.block(\"relu\"):\n",
    "        vn, vi, vj = T.axis.remap(\"SSS\", [n, i, j])\n",
    "        C[vn, vi, vj] = T.max(intermediate[vn, vi, vj], T.float32(0))\n",
    "\n",
    "print(MyBmmRelu.script())\n",
    "#sch = tvm.tir.Schedule(MyBmmRelu)\n",
    "#IPython.display.Code(sch.mod.script(), language=\"python\")\n",
    "# Also please validate your result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# from tvm.script import tir as T\n",
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def bmm_relu(A: T.Buffer[(16, 128, 128), \"float32\"], B: T.Buffer[(16, 128, 128), \"float32\"], C: T.Buffer[(16, 128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with T.block(\"root\")\n",
      "        intermediate = T.alloc_buffer([16, 128, 128], dtype=\"float32\")\n",
      "        for n in T.parallel(16):\n",
      "            for ax0, ax1, ax2 in T.grid(128, 128, 128):\n",
      "                with T.block(\"matmul\"):\n",
      "                    vn, vi, vj, vk = T.axis.remap(\"SSSR\", [n, ax0, ax1, ax2])\n",
      "                    T.reads(A[vn, vi, vk], B[vn, vk, vj])\n",
      "                    T.writes(intermediate[vn, vi, vj])\n",
      "                    with T.init():\n",
      "                        intermediate[vn, vi, vj] = T.float32(0)\n",
      "                    intermediate[vn, vi, vj] = intermediate[vn, vi, vj] + A[vn, vi, vk] * B[vn, vk, vj]\n",
      "            for i, j in T.grid(128, 128):\n",
      "                with T.block(\"relu\"):\n",
      "                    vn, vi, vj = T.axis.remap(\"SSS\", [n, i, j])\n",
      "                    T.reads(intermediate[vn, vi, vj])\n",
      "                    T.writes(C[vn, vi, vj])\n",
      "                    C[vn, vi, vj] = T.max(intermediate[vn, vi, vj], T.float32(0))\n",
      "    \n",
      "\n",
      "**************************************************\n",
      "# from tvm.script import tir as T\n",
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def bmm_relu(A: T.Buffer[(16, 128, 128), \"float32\"], B: T.Buffer[(16, 128, 128), \"float32\"], C: T.Buffer[(16, 128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with T.block(\"root\")\n",
      "        intermediate = T.alloc_buffer([16, 128, 128], dtype=\"float32\")\n",
      "        for n_init in T.parallel(16):\n",
      "            for ax0_init, ax1_init in T.grid(128, 128):\n",
      "                with T.block(\"matmul_init\"):\n",
      "                    vn, vi, vj = T.axis.remap(\"SSS\", [n_init, ax0_init, ax1_init])\n",
      "                    T.reads()\n",
      "                    T.writes(intermediate[vn, vi, vj])\n",
      "                    intermediate[vn, vi, vj] = T.float32(0)\n",
      "        for n in T.parallel(16):\n",
      "            for ax0, ax1, ax2 in T.grid(128, 128, 128):\n",
      "                with T.block(\"matmul_update\"):\n",
      "                    vn, vi, vj, vk = T.axis.remap(\"SSSR\", [n, ax0, ax1, ax2])\n",
      "                    T.reads(intermediate[vn, vi, vj], A[vn, vi, vk], B[vn, vk, vj])\n",
      "                    T.writes(intermediate[vn, vi, vj])\n",
      "                    intermediate[vn, vi, vj] = intermediate[vn, vi, vj] + A[vn, vi, vk] * B[vn, vk, vj]\n",
      "            for i, j in T.grid(128, 128):\n",
      "                with T.block(\"relu\"):\n",
      "                    vn, vi, vj = T.axis.remap(\"SSS\", [n, i, j])\n",
      "                    T.reads(intermediate[vn, vi, vj])\n",
      "                    T.writes(C[vn, vi, vj])\n",
      "                    C[vn, vi, vj] = T.max(intermediate[vn, vi, vj], T.float32(0))\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sch = tvm.tir.Schedule(MyBmmRelu)\n",
    "\n",
    "matmul_block = sch.get_block(\"matmul\", func_name=\"bmm_relu\")\n",
    "matmul_n, matmul_i, matmul_j, matmul_k = sch.get_loops(matmul_block)\n",
    "\n",
    "#k0, k1 = sch.split(matmul_k, [8, 16])\n",
    "# sch.reorder(k1, k0)\n",
    "\n",
    "res = sch.get_loops(sch.get_block(\"relu\", func_name=\"bmm_relu\"))\n",
    "sch.compute_at(matmul_block, res[0])\n",
    "sch.parallel(res[0])\n",
    "\n",
    "matmul_n, matmul_i, matmul_j, matmul_k = sch.get_loops(matmul_block)\n",
    "\n",
    "print(sch.mod.script())\n",
    "print('*' * 50)\n",
    "sch.decompose_reduction(matmul_block, matmul_n)\n",
    "\n",
    "'''\n",
    "# TODO: transformations\n",
    "# Hints: you can use\n",
    "# `IPython.display.Code(sch.mod.script(), language=\"python\")`\n",
    "# or `print(sch.mod.script())`\n",
    "# to show the current program at any time during the transformation.\n",
    "\n",
    "# Step 1. Get blocks\n",
    "Y = sch.get_block(\"Y\", func_name=\"bmm_relu\")\n",
    "...\n",
    "\n",
    "# Step 2. Get loops\n",
    "b, i, j, k = sch.get_loops(Y)\n",
    "...\n",
    "\n",
    "# Step 3. Organize the loops\n",
    "k0, k1 = sch.split(k, ...)\n",
    "sch.reorder(...)\n",
    "sch.compute_at/reverse_compute_at(...)\n",
    "...\n",
    "\n",
    "# Step 4. decompose reduction\n",
    "Y_init = sch.decompose_reduction(Y, ...)\n",
    "...\n",
    "\n",
    "# Step 5. vectorize / parallel / unroll\n",
    "sch.vectorize(...)\n",
    "sch.parallel(...)\n",
    "sch.unroll(...)\n",
    "...\n",
    "\n",
    "IPython.display.Code(sch.mod.script(), language=\"python\")\n",
    "'''\n",
    "\n",
    "print(sch.mod.script())\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ed0fcb194da061c7bddbe287376eaed9b715e5ca5b37233de3efdea51679c08"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
